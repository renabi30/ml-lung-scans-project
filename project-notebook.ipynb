{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Research Project\n",
        "\n",
        "\n",
        "\n",
        "## Name: Rena Bi (rbi4), Sydney Friedel (sfriede5), Varun Harish (vharish1)"
      ],
      "metadata": {
        "id": "j5Qhnl8StlXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess the data\n"
      ],
      "metadata": {
        "id": "RDE6kXMXuB9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if not uploaded, upload lung-scans-dataset.zip\n",
        "# this unzips the file to folder lung-scans-dataset\n",
        "!jar xvf /content/lung-scans-dataset.zip"
      ],
      "metadata": {
        "id": "6WAU2Dz4tt1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the different image sets\n",
        "covid_path=\"lung-scans-dataset/covid\" # 256 x 256 \n",
        "lung_op_path=\"lung-scans-dataset/lung-opacity\" # 256 x 256\n",
        "normal1_path=\"lung-scans-dataset/normal-1\" # 256 x 256\n",
        "normal2_path=\"lung-scans-dataset/normal-2\" # 512 x 512\n",
        "tuber_path=\"lung-scans-dataset/tuberculosis\" # 512 x 512\n",
        "pneum_path=\"lung-scans-dataset/viral-pneumonia\" # 256 x 256"
      ],
      "metadata": {
        "id": "kXaIp9QXG4NC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "EJIWM-aUKiev"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert covid images into numpy arrays\n",
        "covid_X = np.empty((1, 256, 256, 3), dtype=np.uint8)\n",
        "covid_y = np.empty(1) # 0\n",
        "\n",
        "for img in os.listdir(covid_path):\n",
        "  image_path=covid_path+\"/\"+img\n",
        "  covid_X.append(np.array(Image.open(image_path)))\n",
        "  covid_y.append(0)\n",
        "\n",
        "print(covid_X.shape)\n",
        "PIL_image = Image.fromarray(np.uint8(covid_X[0]))\n",
        "display(PIL_image)"
      ],
      "metadata": {
        "id": "g_jNvdewJAmc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "02d2fddd-36f6-4c44-cc53-d692a197ba25"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 256, 256, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x256 at 0x7FA3B68AD790>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAAAA2klEQVR4nO3BMQEAAAjAoD32sP9tQIsAtTcBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFUP/eIA2vuzxOEAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lung_op_X = np.empty((256, 256, 3), dtype=np.int8)\n",
        "lung_op_y = np.empty(1) # 1\n",
        "\n",
        "for img in os.listdir(lung_op_path):\n",
        "  lung_op_X.append(np.array(Image.open(img)))\n",
        "  lung_op_y.append(1)"
      ],
      "metadata": {
        "id": "AgTKl4LfUOOz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal1_X = np.empty((256, 256, 3), dtype=np.int8)\n",
        "normal2_X = np.empty((256, 256, 3), dtype=np.int8) # originally 512 x 512\n",
        "normal_y = np.empty(1) # 2\n",
        "\n",
        "for img in os.listdir(normal1_path):\n",
        "  normal1_X.append(np.array(Image.open(img)))\n",
        "\n",
        "for img in os.listdir(normal2_path):\n",
        "  image = Image.open(img) \n",
        "  scaled_img = image.resize((256, 256))\n",
        "  normal2_X.append(np.array(scaled_img))\n",
        "\n",
        "# remove duplicate images in the normal chest scans \n",
        "# since from two different datasets, but each referenced the same source\n",
        "normal_X = np.stack(normal1_X, normal2_X)\n",
        "normal_X = np.unique(normal_X)\n",
        "\n",
        "for x in normal_X:\n",
        "  normal_y.append(2)"
      ],
      "metadata": {
        "id": "DmIsuBfRLSqC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "4193e927-9dba-4e64-a7a8-a0c5fd8921ea"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-70a5109c1615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal2_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mscaled_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mnormal2_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Normal-3482.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tbc_X = np.empty((256, 256, 3), dtype=np.int8)\n",
        "tbc_X = np.empty(1) # 3\n",
        "\n"
      ],
      "metadata": {
        "id": "b5vRvCopUYh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pneum_X= np.empty((256, 256, 3), dtype=np.int8)\n",
        "pneum_y = np.empty(1) # 4\n"
      ],
      "metadata": {
        "id": "23bSC_KDUS9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train/test/val split for each category and label data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "...     X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "pkgasJw2MYYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline CNN Models for Medical Image Classification"
      ],
      "metadata": {
        "id": "rz5WzVbkTfk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Conv2D,Flatten,MaxPooling2D, Dropout, GlobalMaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import RMSprop,SGD,Adam\n",
        "from tensorflow.keras.applications import VGG16\n",
        "img_width = 224 # CHANGE\n",
        "img_height = 224 # CHANGE\n",
        "\n",
        "#model 0: very simple CNN from scratch\n",
        "model0 = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height,1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(4, activation = \"softmax\")\n",
        "])\n",
        "model0.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "#model 1: more complex CNN from scratch - uses dropout layers for regularization\n",
        "#hyperparameter tuning: kernel size (try (2,2) and (4,4)), num_filters (try removing initial 16 layer and subsequent maxpooling layer), maxpooling size (try (3,3) and (4,4))\n",
        "model1 = Sequential([\n",
        "    Conv2D(16,(3,3),activation = \"relu\" , input_shape = (img_width,img_height,1)) ,\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(32,(3,3),activation = \"relu\") ,  \n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64,(3,3),activation = \"relu\") ,  \n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(128,(3,3),activation = \"relu\"),  \n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(), \n",
        "    Dense(550,activation=\"relu\"),      #Adding the Hidden layer\n",
        "    Dropout(0.1,seed = 2019),\n",
        "    Dense(400,activation =\"relu\"),\n",
        "    Dropout(0.3,seed = 2019),\n",
        "    Dense(300,activation=\"relu\"),\n",
        "    Dropout(0.4,seed = 2019),\n",
        "    Dense(200,activation =\"relu\"),\n",
        "    Dropout(0.2,seed = 2019),\n",
        "    Dense(4,activation = \"softmax\")   #Adding the Output Layer\n",
        "])\n",
        "\n",
        "#hyperparameter tuning: different optimizers (SGD, RMSprop) and learning rates (a few different ones depending on the optimizer)\n",
        "adam=Adam(learning_rate=0.001)\n",
        "model1.compile(optimizer=adam, loss='categorical_crossentropy', metrics = ['acc'])\n",
        "\n",
        "#model 2: AlexNet architecture\n",
        "model2 = Sequential([\n",
        "    Conv2D(filters=128, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(img_width, img_height, 1)),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size=(2,2)),\n",
        "    Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size=(3,3)),\n",
        "    Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D(pool_size=(2,2)),\n",
        "    Flatten(),\n",
        "    Dense(1024,activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1024,activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(4,activation='softmax')    \n",
        "])\n",
        "model2.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=SGD(lr=0.001),\n",
        "    metrics=['accuracy']    \n",
        ")\n",
        "\n",
        "#model 3: VGG16 with added training layers (transfer learning)\n",
        "pre_trained_model = VGG16(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\")\n",
        "#freezing layers\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False\n",
        "last_layer = pre_trained_model.get_layer('block5_pool')\n",
        "last_output = last_layer.output\n",
        "x = GlobalMaxPooling2D()(last_output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(4, activation='softmax')(x)\n",
        "model3 = Model(pre_trained_model.input, x)\n",
        "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n"
      ],
      "metadata": {
        "id": "XfxRB8rNTyBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization Components and Training the Models (do hyperparameter tuning here)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XZPYPJerZH8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#experiment with using these or not using them\n",
        "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)\n",
        "callbacks_list = [ early, learning_rate_reduction]\n",
        "\n",
        "#weighting classes to account for minority classes (diseased) versus majority class (normal)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "weights = compute_class_weight('balanced', np.unique(train.classes), train.classes)\n",
        "cw = dict(zip( np.unique(train.classes), weights))\n",
        "print(cw)\n",
        "\n",
        "#train and evaluate each model on test set\n",
        "#also get predictions for future confusion matrices\n",
        "#hyperparameter tuning: number epochs, weight or unweight, remove regularization (callback list)\n",
        "n_epochs = 50\n",
        "batch_size = 10\n",
        "\n",
        "model0_history = model0.fit(train,epochs=n_epochs, validation_data=val, class_weight=cw, callbacks=callbacks_list)\n",
        "test_loss, test_acc = model0.evaluate(test)\n",
        "model0_preds = model_0.predict(test)\n",
        "\n",
        "model1_history = model1.fit(train,epochs=n_epochs, validation_data=val, class_weight=cw, callbacks=callbacks_list)\n",
        "test_loss, test_acc = model1.evaluate(test)\n",
        "model1_preds = model_0.predict(test)\n",
        "\n",
        "\n",
        "model2_history = model2.fit(train,epochs=n_epochs,validation_data=val,  class_weight=cw, callbacks=callbacks_list)\n",
        "test_loss, test_acc = model2.evaluate(test)\n",
        "model2_preds = model_0.predict(test)\n",
        "\n",
        "model3_history = model3.fit(train,epochs = n_epochs,validation_data=val, class_weight=cw, callbacks=callbacks_list)\n",
        "test_loss, test_acc = model3.evaluate(test)\n",
        "model3_preds = model_0.predict(test)\n",
        "\n"
      ],
      "metadata": {
        "id": "hI58HKZfZjDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy Plots, Confusion Matrices, F-Beta Score Computations (TODO)"
      ],
      "metadata": {
        "id": "1zjpkR9fP-pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison of Best CNN with SVM and Decision Tree Models for Image Classification"
      ],
      "metadata": {
        "id": "P4kns3DnQYtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining and evaluating SVM classifier\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid={'C':[0.1,1,10,100],'gamma':[0.0001,0.001,0.1,1],'kernel':['rbf','poly']}\n",
        "svc=svm.SVC(probability=True)\n",
        "svm_model=GridSearchCV(svc,param_grid)\n",
        "\n",
        "svm_model.fit(train)\n",
        "# model.best_params_ contains the best parameters obtained from GridSearchCV\n",
        "svm_pred=model.predict(test)\n",
        "\n",
        "#defining and evaluating classfier that uses CNN for feature extraction then SVM for classification\n",
        "svm_cnn_model = Sequential([\n",
        "    Conv2D(filters = 32, padding = \"same\",activation = \"relu\",kernel_size=3, strides = 2,input_shape=(img_width, img_height, 1)),\n",
        "    Conv2D(filters = 32, padding = \"same\",activation = \"relu\",kernel_size=3, strides = 2,input_shape=(img_width, img_height, 1)),\n",
        "    MaxPool2D(pool_size=(2,2),strides = 2),\n",
        "    Conv2D(filters = 32, padding = \"same\",activation = \"relu\",kernel_size=3),\n",
        "    MaxPool2D(pool_size=(2,2),strides = 2),\n",
        "    Flatten(),\n",
        "    (Dense(128,activation=\"relu\")\n",
        "])\n",
        "#svm output layer\n",
        "n_classes = 4\n",
        "svm_cnn_model.add(Dense(n_classes,kernel_regularizers = l2(0.01),activation= “softmax”))\n",
        "svm_cnn_model.compile(optimizer=”adam”,loss=”squared_hinge”, metrics = [‘accuracy’])\n",
        "svm_cnn_model_history = svm_cnn_model.fit(x = train, validation_data = val)\n",
        "\n",
        "\n",
        "# another version\n",
        "#the base model/feature extractor using a CNN\n",
        "feature_extractor = Sequential([\n",
        "  Conv2D(16,(5,5),padding='valid',input_shape = X_train.shape[1:]),\n",
        "  Activation('relu'),\n",
        "  MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'),\n",
        "  Dropout(0.4),\n",
        "  Conv2D(32,(5,5),padding='valid'),\n",
        "  Activation('relu'),\n",
        "  MaxPooling2D(pool_size=(2,2),strides=2,padding = 'valid'),\n",
        "  Dropout(0.6),\n",
        "  Conv2D(64,(5,5),padding='valid'),\n",
        "  Activation('relu'),\n",
        "  Dropout(0.8),\n",
        "  Flatten(),\n",
        "  Dense(2),\n",
        "  Activation('softmax'),\n",
        "])\n",
        "\n",
        "#feature extraction\n",
        "model_feat = Model(inputs=feature_extractor.input,outputs=feature_extractor.get_layer('dense_1').output)\n",
        "train_features = model_feat.predict(train)\n",
        "test_features = model_feat.predict(test)\n",
        "\n",
        "#fitting of SVM as a classifier and evaluating on test set\n",
        "svm = SVC(kernel='rbf')\n",
        "svm.fit(train_features,np.argmax(y_train,axis=1))\n",
        "svm.score(test_features,np.argmax(y_test,axis=1))\n",
        "\n",
        "\n",
        "# defining decision tree classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_model=RandomForestClassifier()\n",
        "rf_model.fit(train)\n",
        "rf_pred=model.predict(test)\n",
        "\n",
        "\n",
        "#another version\n",
        "#feature extractor with CNN (above)\n",
        "#decision tree classifier\n",
        "dt = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,max_depth=3, min_samples_leaf=5)\n",
        "dt.fit(train_features,np.argmax(y_train,axis=1))\n",
        "dt.score(test_features,np.argmax(y_test,axis=1))"
      ],
      "metadata": {
        "id": "NDUs8LGeSKPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Filters and Feature Maps of Best Performing Model\n"
      ],
      "metadata": {
        "id": "RBzF9_ToTqhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing filters for early hidden layers\n",
        "## TODO: use best_model\n",
        "\n",
        "# retrieve weights from the second hidden layer\n",
        "# cannot easily visualize filters lower down\n",
        "from matplotlib import pyplot\n",
        "filters, biases = best_model.layers[1].get_weights()\n",
        "# normalize filter values to 0-1 so we can visualize them\n",
        "f_min, f_max = filters.min(), filters.max()\n",
        "filters = (filters - f_min) / (f_max - f_min)\n",
        "# plot first few filters\n",
        "n_filters, ix = 6, 1\n",
        "for i in range(n_filters):\n",
        " # get the filter\n",
        " f = filters[:, :, :, i]\n",
        " # plot each channel separately\n",
        " for j in range(3):\n",
        " # specify subplot and turn of axis\n",
        " ax = pyplot.subplot(n_filters, 3, ix)\n",
        " ax.set_xticks([])\n",
        " ax.set_yticks([])\n",
        " # plot filter channel in grayscale\n",
        " pyplot.imshow(f[:, :, j], cmap='gray')\n",
        " ix += 1\n",
        "# show the figure\n",
        "pyplot.show()\n",
        "\n",
        "#visualizing feature maps\n",
        "# plot feature map of first conv layer for given image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import Model\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "# redefine model to output right after the first hidden layer\n",
        "short_model = Model(inputs=best_model.inputs, outputs=best_model.layers[1].output)\n",
        "short_model.summary()\n",
        "# load a test image with the required shape\n",
        "img = load_img('img.jpg', target_size=(img_width, img_height))\n",
        "# convert the image to an array\n",
        "img = img_to_array(img)\n",
        "# expand dimensions so that it represents a single 'sample'\n",
        "img = expand_dims(img, axis=0)\n",
        "# prepare the image (e.g. scale pixel values for the vgg)\n",
        "img = preprocess_input(img)\n",
        "# get feature map for first hidden layer\n",
        "feature_maps = short_model.predict(img)\n",
        "# plot all 64 maps in an 8x8 squares #TODO: change size to best_model's dimensions (or just keep as VGG16 for demonstration purposes?)\n",
        "square = 8\n",
        "ix = 1\n",
        "for _ in range(square):\n",
        " for _ in range(square):\n",
        " # specify subplot and turn of axis\n",
        " ax = pyplot.subplot(square, square, ix)\n",
        " ax.set_xticks([])\n",
        " ax.set_yticks([])\n",
        " # plot filter channel in grayscale\n",
        " pyplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
        " ix += 1\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "z6Ry-C06T0KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Would Like to Completes) - Performing Histogram Equalization and Seeing How Performance of Best Model Changes"
      ],
      "metadata": {
        "id": "zt-AE7P9Pmg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# histogram equalization function\n",
        "def hist(img):\n",
        "  img_to_yuv = cv2.cvtColor(img,cv2.COLOR_BGR2YUV)\n",
        "  img_to_yuv[:,:,0] = cv2.equalizeHist(img_to_yuv[:,:,0])\n",
        "  hist_equalization_result = cv2.cvtColor(img_to_yuv, cv2.COLOR_YUV2BGR)\n",
        "  return hist_equalization_result\n",
        "\n",
        "img = cv2.imread('img.jpg',0)\n"
      ],
      "metadata": {
        "id": "OsUJWO3pPlLO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}